# MIÂ²RedTeam  {-}

The MI2.AI Red Team analyses predictive models and other artificial intelligence solutions, identifies vulnerabilities and weaknesses in order to improve its security and robustness. Also develops methods and tools for explanatory model analysis (EMA).

####  {-}

Our red team is a group of experienced XAI researchers who look for vulnerabilities and weaknesses in order to improve its security and robustness of AI solutions. We use state-of-the-art methods and develop new methods and software to test the security and robustness of predictive models. 

There are many different ways that our red team can test the resilience of an AI system, depending on the specific goals and objectives of the exercise. Some common techniques used by our red team include:

- Testing **the robustness of an AI system to adversarial examples**: An adversarial example is an input to an AI system that has been modified in some way to cause the system to make a wrong prediction, in order to test the system's robustness.
- Testing **the security of an AI system** by attempting to access or modify data, or reverse engineering the training data.
- Testing **the robustness of an AI system to changing conditions**, such as introducing noise or introducing new data that the system has not seen before.

Overall, the goal of our red team in the context of AI is to identify and exploit weaknesses in the system in order to improve the system's robustness and security. This can help organizations to better understand the vulnerabilities of their AI systems and take steps to mitigate them.

**Example solutions** developed by our red team:

- [DALEX](https://dalex.drwhy.ai/), [modelStudio](https://github.com/ModelOriented/modelStudio), [InteractiveEMA](https://arxiv.org/abs/2005.00497) for exploration of models for tabular data
- [SurvSHAP](https://arxiv.org/abs/2208.11080) for exploration of survival models
- [TreeSHAP](https://github.com/ModelOriented/treeshap), [randomForestExplainer](https://github.com/ModelOriented/randomForestExplainer), [EIX](https://github.com/ModelOriented/EIX) for exploration of tree models
- [drifter](https://github.com/ModelOriented/drifter) for concept drift analysis
- [fairmodels](https://fairmodels.drwhy.ai/) for fairness analysis
- [Models in the Wild](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_20) for exploration of language models
- [LIMEcraft](https://link.springer.com/article/10.1007/s10994-022-06204-w), [Checklist](https://www.sciencedirect.com/science/article/pii/S0031320321002223) for exploration of models for image data
